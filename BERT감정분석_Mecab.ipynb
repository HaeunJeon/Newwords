{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT감정분석_Mecab.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ZjWLFTwvdriL",
        "Xp6bkY_xgBbO",
        "w3U4CjSsgmEO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HaeunJeon/Newwords/blob/main/BERT%EA%B0%90%EC%A0%95%EB%B6%84%EC%84%9D_Mecab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxhvy05ZFk9V",
        "outputId": "d2f5559e-fbe1-488d-c10f-b4b1de7fb7f8"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpCSCqk4Fqzm"
      },
      "source": [
        "import tensorflow as tf\n",
        "import torch\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lqXCZOIFw2O",
        "outputId": "0a02b788-392a-46c9-850e-d055980a7a74"
      },
      "source": [
        "# 네이버 영화리뷰 감정분석 데이터 다운로드\n",
        "!git clone https://github.com/e9t/nsmc.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nsmc'...\n",
            "remote: Enumerating objects: 14763, done.\u001b[K\n",
            "remote: Total 14763 (delta 0), reused 0 (delta 0), pack-reused 14763\u001b[K\n",
            "Receiving objects: 100% (14763/14763), 56.19 MiB | 21.85 MiB/s, done.\n",
            "Resolving deltas: 100% (1749/1749), done.\n",
            "Checking out files: 100% (14737/14737), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcYqHr6P3VQd"
      },
      "source": [
        "# 판다스 라이브러리 사용\n",
        "import pandas as pd\n",
        "file1 = pd.read_csv('/content/최종sentiment_labled_train.csv')\n",
        "file2 = pd.read_csv('/content/지금sentiment_labeled_test.csv')\n",
        "file1.columns = ['id', 'document', 'label']\n",
        "file2.columns = ['id', 'document', 'label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8prRFyvF3a0",
        "outputId": "95a71837-5727-4278-a91a-41c9ce8989f5"
      },
      "source": [
        "# 판다스로 훈련셋과 테스트셋 데이터 로드\n",
        "train = pd.read_csv(\"nsmc/ratings_train.txt\", sep='\\t')\n",
        "test = pd.read_csv(\"nsmc/ratings_test.txt\", sep='\\t')\n",
        "train = pd.concat([train[:150000],file1])\n",
        "test = pd.concat([test[:50000],file2])\n",
        "print(train.shape)#15만\n",
        "print(test.shape)#5만"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150999, 3)\n",
            "(50428, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UStaVv65Im5v"
      },
      "source": [
        "train = train[train['document'].notna()]\n",
        "test = test[test['document'].notna()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vsh284IpM6Y2"
      },
      "source": [
        "train['document'] = train['document'].apply(lambda x: str(x))\n",
        "test['document'] = test['document'].apply(lambda x: str(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "RzU8gKPhF6El",
        "outputId": "bff23267-1e95-482a-b7ab-8f3654e90e84"
      },
      "source": [
        "# 훈련셋의 앞부분 출력\n",
        "train.tail(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>996</td>\n",
              "      <td>페이커는 지금이 최전성기임 ㅇㅇ</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>997</td>\n",
              "      <td>상향평준화라고 말하는 담갈 페까 수준.JPG</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991</th>\n",
              "      <td>998</td>\n",
              "      <td>갓트남... 선진의료기술로 털바퀴 중성화...avi</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992</th>\n",
              "      <td>999</td>\n",
              "      <td>쵸비 이 병신새끼 왜 페이커만 만나면 쳐발리냐?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>1000</td>\n",
              "      <td>챌코 개좆슼 왤케 못하냐</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>1001</td>\n",
              "      <td>양즙대전 빨리보고싶어 부들부들떨린다</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>1002</td>\n",
              "      <td>메시 왜나갔냐 보니까 진짜 팬들 다떠나겠더라ㅋ</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>1003</td>\n",
              "      <td>근데 페이커 퍼스트논란 존나어이없지 않냐?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>1004</td>\n",
              "      <td>개좆즙갈 논리면 칸나&gt;기인임</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>1005</td>\n",
              "      <td>룰러맘들이 ㄹㅇ 역겨웠던게</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id                      document  label\n",
              "989   996             페이커는 지금이 최전성기임 ㅇㅇ      1\n",
              "990   997      상향평준화라고 말하는 담갈 페까 수준.JPG      0\n",
              "991   998  갓트남... 선진의료기술로 털바퀴 중성화...avi      1\n",
              "992   999    쵸비 이 병신새끼 왜 페이커만 만나면 쳐발리냐?      0\n",
              "993  1000                 챌코 개좆슼 왤케 못하냐      0\n",
              "994  1001           양즙대전 빨리보고싶어 부들부들떨린다      1\n",
              "995  1002     메시 왜나갔냐 보니까 진짜 팬들 다떠나겠더라ㅋ      0\n",
              "996  1003       근데 페이커 퍼스트논란 존나어이없지 않냐?      0\n",
              "997  1004               개좆즙갈 논리면 칸나>기인임      0\n",
              "998  1005                룰러맘들이 ㄹㅇ 역겨웠던게      0"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeNazXAbEV2M",
        "outputId": "22e640a1-aab7-49d0-e526-e5bd104402a5"
      },
      "source": [
        "!set -x \\\n",
        "&& pip install konlpy \\\n",
        "&& curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh | bash -x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+ pip install konlpy\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "+ curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh\n",
            "+ bash -x\n",
            "+ mecab_dicdir=/usr/local/lib/mecab/dic/mecab-ko-dic\n",
            "+ set -e\n",
            "++ uname\n",
            "+ os=Linux\n",
            "+ [[ ! Linux == \\L\\i\\n\\u\\x ]]\n",
            "+ hash sudo\n",
            "+ sudo=sudo\n",
            "+ python=python3\n",
            "+ hash pyenv\n",
            "+ at_user_site=\n",
            "++ check_python_site_location_is_writable\n",
            "++ python3 -\n",
            "+ [[ 1 == \\0 ]]\n",
            "+ hash automake\n",
            "+ hash mecab\n",
            "+ echo 'mecab-ko is already installed'\n",
            "mecab-ko is already installed\n",
            "+ [[ -d /usr/local/lib/mecab/dic/mecab-ko-dic ]]\n",
            "+ echo 'mecab-ko-dic is already installed'\n",
            "mecab-ko-dic is already installed\n",
            "++ python3 -c 'import pkgutil; print(1 if pkgutil.find_loader(\"MeCab\") else 0)'\n",
            "+ [[ 1 == \\1 ]]\n",
            "+ echo 'mecab-python is already installed'\n",
            "mecab-python is already installed\n",
            "+ echo Done.\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GdZxr3BCdWl"
      },
      "source": [
        "# Mecab 사용자 사전 추가"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isdm31kRCh3h",
        "outputId": "11a4cfe3-8950-4a08-e154-707dc8c6e192"
      },
      "source": [
        "from konlpy.tag import Mecab \n",
        "mecab = Mecab() \n",
        "print(mecab.pos(\"솜씨좋은장씨의 개발블로그\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('솜씨', 'NNG'), ('좋', 'VA'), ('은', 'ETM'), ('장', 'NNP'), ('씨', 'NNB'), ('의', 'JKG'), ('개발', 'NNG'), ('블로그', 'NNG')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKr6rNgrC6eo",
        "outputId": "450004be-66e0-4940-8d5a-6b6f32cac951"
      },
      "source": [
        "cd /content/mecab-ko-dic-2.1.1-20180720"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mecab-ko-dic-2.1.1-20180720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_0q7x8BDKN4",
        "outputId": "13d5995e-9576-4b63-ca9f-83cbde137e9a"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclocal.m4       feature.def                 model.def          right-id.def\n",
            "AUTHORS          Foreign.csv                 NEWS               Symbol.csv\n",
            "\u001b[0m\u001b[01;32mautogen.sh\u001b[0m*      Group.csv                   NNBC.csv           \u001b[01;34mtools\u001b[0m/\n",
            "\u001b[01;34mautom4te.cache\u001b[0m/  Hanja.csv                   NNB.csv            unk.def\n",
            "ChangeLog        IC.csv                      NNG.csv            \u001b[01;34muser-dic\u001b[0m/\n",
            "char.def         Inflect.csv                 NNP.csv            VA.csv\n",
            "\u001b[01;32mclean\u001b[0m*           INSTALL                     NorthKorea.csv     VCN.csv\n",
            "CoinedWord.csv   \u001b[01;32minstall-sh\u001b[0m*                 NP.csv             VCP.csv\n",
            "config.log       J.csv                       NR.csv             VV.csv\n",
            "\u001b[01;32mconfig.status\u001b[0m*   left-id.def                 \u001b[01;34mnsmc\u001b[0m/              VX.csv\n",
            "\u001b[01;32mconfigure\u001b[0m*       MAG.csv                     Person-actor.csv   Wikipedia.csv\n",
            "configure.ac     MAJ.csv                     Person.csv         XPN.csv\n",
            "COPYING          Makefile                    Place-address.csv  XR.csv\n",
            "dicrc            Makefile.am                 Place.csv          XSA.csv\n",
            "EC.csv           Makefile.in                 Place-station.csv  XSN.csv\n",
            "EF.csv           matrix.def                  pos-id.def         XSV.csv\n",
            "EP.csv           \u001b[01;34mMecab-ko-for-Google-Colab\u001b[0m/  Preanalysis.csv\n",
            "ETM.csv          \u001b[01;32mmissing\u001b[0m*                    README\n",
            "ETN.csv          MM.csv                      rewrite.def\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN7iOLQZDUuV",
        "outputId": "10df011a-eb96-430e-891f-03fb5b5c7a1d"
      },
      "source": [
        "ls user-dic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nnp.csv  person.csv  place.csv  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "YJpnEWwmFmE7",
        "outputId": "fb34a1f3-9fc5-4667-9e6c-e5a1ea27fe36"
      },
      "source": [
        "slangs = pd.read_csv(\"/content/total_answers.csv\")\n",
        "slangs.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>new_words</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>결정장애</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>팔아먹</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>키차이</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  new_words\n",
              "0      결정장애\n",
              "1       팔아먹\n",
              "2       키차이"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "Q3cWCp9TGzU2",
        "outputId": "2e685013-845e-48d2-cfa1-05563c248741"
      },
      "source": [
        "nnp = pd.read_csv(\"/content/mecab-ko-dic-2.1.1-20180720/user-dic/nnp.csv\")\n",
        "nnp\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>대우</th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "      <th>Unnamed: 3</th>\n",
              "      <th>NNP</th>\n",
              "      <th>*</th>\n",
              "      <th>F</th>\n",
              "      <th>대우.1</th>\n",
              "      <th>*.1</th>\n",
              "      <th>*.2</th>\n",
              "      <th>*.3</th>\n",
              "      <th>*.4</th>\n",
              "      <th>*.5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>구글</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NNP</td>\n",
              "      <td>*</td>\n",
              "      <td>T</td>\n",
              "      <td>구글</td>\n",
              "      <td>*</td>\n",
              "      <td>*</td>\n",
              "      <td>*</td>\n",
              "      <td>*</td>\n",
              "      <td>*</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   대우  Unnamed: 1  Unnamed: 2  Unnamed: 3  NNP  *  F 대우.1 *.1 *.2 *.3 *.4 *.5\n",
              "0  구글         NaN         NaN         NaN  NNP  *  T   구글   *   *   *   *   *"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19BRi6Q7G3Eg",
        "outputId": "43492831-a4b3-4392-8532-5c67f6e407b7"
      },
      "source": [
        "word_list = slangs.new_words\n",
        "word_list.to_list()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['결정장애',\n",
              " '팔아먹',\n",
              " '키차이',\n",
              " '심리테스트',\n",
              " '놀이기구',\n",
              " '탄핵가자',\n",
              " '온도차',\n",
              " '법정구속',\n",
              " '즈그',\n",
              " '측근정리',\n",
              " '잡아떼',\n",
              " '싸이',\n",
              " '민주당',\n",
              " '기상캐스터',\n",
              " '밀어주',\n",
              " '마이크로소프트',\n",
              " '페미',\n",
              " '북한인권문제',\n",
              " '돈까스',\n",
              " '탄핵집회',\n",
              " '팬싸인회',\n",
              " '우병우라인',\n",
              " '라이젠',\n",
              " '몇일전',\n",
              " '에르메스',\n",
              " '국정농단',\n",
              " '메가박스',\n",
              " '살인사건',\n",
              " '악플',\n",
              " '예능프로그램',\n",
              " '직장상사',\n",
              " '더쿠',\n",
              " '차명계좌',\n",
              " '임진왜란때',\n",
              " '몸상태',\n",
              " '논란중인',\n",
              " '역사왜곡',\n",
              " '종북좌빨',\n",
              " '북한군',\n",
              " '고민중이',\n",
              " '녀언',\n",
              " '가짜뉴스',\n",
              " '빼박',\n",
              " '챙기',\n",
              " '넘사벽',\n",
              " '구원파',\n",
              " '치트키',\n",
              " '닭근혜가',\n",
              " '서울경기',\n",
              " '여자배구',\n",
              " '헤드셋',\n",
              " '외신기자',\n",
              " '장동민',\n",
              " '선택장애',\n",
              " '나타나',\n",
              " '챙피하다',\n",
              " '질질끌',\n",
              " '쫓아내',\n",
              " '앱스토어',\n",
              " '투표합시다',\n",
              " '싸가지없',\n",
              " '횽들',\n",
              " '티비',\n",
              " '랜덤',\n",
              " '낄낄낄낄',\n",
              " '담배값',\n",
              " '펀딩',\n",
              " '핑계대지',\n",
              " '백남기',\n",
              " '리뉴얼',\n",
              " '사이비교주',\n",
              " '탄핵감',\n",
              " '신경쓰',\n",
              " '슈퍼카',\n",
              " '엄중처벌',\n",
              " '천만원',\n",
              " '놈현',\n",
              " '싹다',\n",
              " '쌩라이브',\n",
              " '케베스',\n",
              " '엠비씨',\n",
              " '거의다',\n",
              " '건강보험료',\n",
              " '어느정도',\n",
              " '양의지',\n",
              " '언제쯤',\n",
              " '해외직구',\n",
              " '히딩크',\n",
              " '눔들',\n",
              " '뜨거',\n",
              " '노브랜드',\n",
              " '브이앱',\n",
              " '끝판왕',\n",
              " '낱낱히',\n",
              " '삼시세끼',\n",
              " '북한인권결의',\n",
              " '플스',\n",
              " '법무법인',\n",
              " '소상공인',\n",
              " '윗대가리',\n",
              " '무단횡단',\n",
              " '훔치',\n",
              " '휩쓸리',\n",
              " '강남역',\n",
              " '백종원의',\n",
              " '임기보장',\n",
              " '꺼내',\n",
              " '트위터',\n",
              " '동북아',\n",
              " '반토막',\n",
              " '쁘',\n",
              " '알라딘',\n",
              " '자영업자',\n",
              " '저여자',\n",
              " '프로듀스',\n",
              " '월드시리즈',\n",
              " '익스',\n",
              " '존나게',\n",
              " '저사람',\n",
              " '믹스',\n",
              " '호빠',\n",
              " '며칠동안',\n",
              " '야당넘들',\n",
              " '손석희님',\n",
              " '이사람',\n",
              " '꿰',\n",
              " '법무장관',\n",
              " '뻘짓거리',\n",
              " '농구선수',\n",
              " '박세리',\n",
              " '징징거리',\n",
              " '축하드립니',\n",
              " '국가보안법위반',\n",
              " '헌법개정',\n",
              " '돌아이',\n",
              " '첫방',\n",
              " '죗값받',\n",
              " '틈타',\n",
              " '방탄소년단',\n",
              " '워마드',\n",
              " '옆동네',\n",
              " '신입사원',\n",
              " '첫방송',\n",
              " '대통령후보',\n",
              " '프린세스',\n",
              " '종북세력',\n",
              " '푹빠',\n",
              " '사이코패스',\n",
              " '왜자꾸',\n",
              " '겟아웃',\n",
              " '졸업사진',\n",
              " '윈도우',\n",
              " '뜸들이',\n",
              " '크루즈',\n",
              " '샌더스',\n",
              " '복지부',\n",
              " '펨',\n",
              " '만천원',\n",
              " '레인보우',\n",
              " '북조선',\n",
              " '헬스장',\n",
              " '쪼개',\n",
              " '물어뜯',\n",
              " '핵잠수함',\n",
              " '쿠키런',\n",
              " '오바마',\n",
              " '프로포폴',\n",
              " '태블릿',\n",
              " '넌센스',\n",
              " '선제타격',\n",
              " '교통방송',\n",
              " '냥아치',\n",
              " '제일먼저',\n",
              " '컵밥',\n",
              " '유병언',\n",
              " '한일군사정보',\n",
              " '롯데월드',\n",
              " '펩시콜라',\n",
              " '엣지',\n",
              " '상대선수',\n",
              " '곧대로',\n",
              " '이나라',\n",
              " '헷갈',\n",
              " '보배드림',\n",
              " '컴플레인',\n",
              " '원내대표',\n",
              " '팬덤',\n",
              " '얼마정도',\n",
              " '양승태',\n",
              " '인어공주',\n",
              " '신조어사전',\n",
              " '간철수',\n",
              " '관악구',\n",
              " '손글씨',\n",
              " '헌법재판관',\n",
              " '탐나',\n",
              " '외교장관',\n",
              " '피해자인',\n",
              " '햐야',\n",
              " '배달음식',\n",
              " '김태희',\n",
              " '티저',\n",
              " '홍진영',\n",
              " '돌발행동',\n",
              " '동물농장',\n",
              " '론스타',\n",
              " '국정정상화',\n",
              " '녹음파일',\n",
              " '짜증나서',\n",
              " '모자이크처리',\n",
              " '죽이',\n",
              " '쥬지',\n",
              " '국민혈세',\n",
              " '슈퍼콘',\n",
              " '롯데',\n",
              " '호위호식',\n",
              " '현대백화점',\n",
              " '가져가',\n",
              " '저희집',\n",
              " '암덩어리',\n",
              " '씌우',\n",
              " '추대표',\n",
              " '바이브',\n",
              " '십년동안',\n",
              " '엎',\n",
              " '영수회담',\n",
              " '메르스',\n",
              " '출석체크',\n",
              " '쪽팔리',\n",
              " '콜오브듀티',\n",
              " '김경문',\n",
              " '김영삼',\n",
              " '헌재',\n",
              " '병원비',\n",
              " '런던올림픽',\n",
              " '박원순',\n",
              " '유투브',\n",
              " '총리임명',\n",
              " '믹서기',\n",
              " '콤보',\n",
              " '소소한',\n",
              " '핵포기',\n",
              " '허경영',\n",
              " '엽기토끼',\n",
              " '측근비리',\n",
              " '여자대통령',\n",
              " '다저스',\n",
              " '벅차',\n",
              " '박나래',\n",
              " '뇌물현',\n",
              " '박명수',\n",
              " '땅투기',\n",
              " '강원랜드',\n",
              " '박근혜정권',\n",
              " '외제차',\n",
              " '딥',\n",
              " '프로포즈',\n",
              " '택시비',\n",
              " '광진구',\n",
              " '우리애',\n",
              " '뿌리뽑',\n",
              " '일본산',\n",
              " '연락두절',\n",
              " '국가기밀',\n",
              " '박근헤',\n",
              " '뭐하나',\n",
              " '삼성폰',\n",
              " '끊기',\n",
              " '퇴근후',\n",
              " '댓글부대',\n",
              " '공중파',\n",
              " '합니',\n",
              " '플라',\n",
              " '침대축구',\n",
              " '반기문',\n",
              " '눈까리',\n",
              " '우유주사',\n",
              " '잡아넣',\n",
              " '피디수첩',\n",
              " '얕보',\n",
              " '촬영중인',\n",
              " '로또',\n",
              " '서태지',\n",
              " '달달한',\n",
              " '왜그러',\n",
              " '색깔논쟁',\n",
              " '한시간',\n",
              " '랑종',\n",
              " '피씨방',\n",
              " '연기대상',\n",
              " '김정민',\n",
              " '비쥬얼',\n",
              " '탕탕절',\n",
              " '윤종신',\n",
              " '콩밥먹',\n",
              " '악성댓글',\n",
              " '에픽하이',\n",
              " '칠푼이칠푼이',\n",
              " '맥그리거',\n",
              " '밍크뮤',\n",
              " '셀카',\n",
              " '통일교가',\n",
              " '반려견',\n",
              " '모바일게임',\n",
              " '숏컷',\n",
              " '초중고',\n",
              " '헬스트레이너',\n",
              " '끝나',\n",
              " '사이비종교가',\n",
              " '뉴라이트',\n",
              " '안철수',\n",
              " '재산몰수',\n",
              " '팩트',\n",
              " '북미회담',\n",
              " '뭔가요',\n",
              " '맴버',\n",
              " '엑셀',\n",
              " '뭡니까',\n",
              " '폰케이스',\n",
              " '자유대한민국',\n",
              " '라디오스타',\n",
              " '디시인',\n",
              " '입장문',\n",
              " '앙투아네트',\n",
              " '전소미',\n",
              " '찡하',\n",
              " '레드카펫',\n",
              " '랄라',\n",
              " '비서진',\n",
              " '돼지바',\n",
              " '꼬라지',\n",
              " '공식발표',\n",
              " '왜이래',\n",
              " '몇십년동안',\n",
              " '틀딱들',\n",
              " '핵추진잠수함',\n",
              " '전자제품',\n",
              " '참교육',\n",
              " '종북좌파',\n",
              " '채용공고',\n",
              " '밝혀지',\n",
              " '치과의사',\n",
              " '품귀현상',\n",
              " '깊숙히',\n",
              " '김준호',\n",
              " '네이버',\n",
              " '대통령감이',\n",
              " '북풍몰이',\n",
              " '냄비근성',\n",
              " '굴러가',\n",
              " '현실판',\n",
              " '태세전환',\n",
              " '프리패스',\n",
              " '껍',\n",
              " '히말라야',\n",
              " '출구조사',\n",
              " '블룸버그',\n",
              " '민정수석',\n",
              " '오케이캐시백',\n",
              " '텔레그램',\n",
              " '줄어들',\n",
              " '랭킹위',\n",
              " '놓치',\n",
              " '노무현전대통령',\n",
              " '몰고가',\n",
              " '맘스터치',\n",
              " '줬으',\n",
              " '박보검',\n",
              " '뜬금포로',\n",
              " '삐약이',\n",
              " '루이비통',\n",
              " '흑역사가',\n",
              " '유체이탈',\n",
              " '국가대표',\n",
              " '데스크탑',\n",
              " '봉하마을',\n",
              " '뱀뱀',\n",
              " '의천도룡기',\n",
              " '김태리',\n",
              " '간장게장',\n",
              " '깍아내리',\n",
              " '남북정상회담',\n",
              " '도쿄올림픽',\n",
              " '좌좀',\n",
              " '이번주',\n",
              " '섹시하',\n",
              " '돌아가',\n",
              " '한달동안',\n",
              " '능력없',\n",
              " '뚱뚱한',\n",
              " '세레머니',\n",
              " '녹취록',\n",
              " '벽보고',\n",
              " '멱살잡',\n",
              " '국정운영',\n",
              " '쟤들',\n",
              " '김일성',\n",
              " '현대통령',\n",
              " '방송사고',\n",
              " '다음주',\n",
              " '뒷통수',\n",
              " '보이그룹',\n",
              " '찔리',\n",
              " '문재앙',\n",
              " '꼼짝못하',\n",
              " '신호위반',\n",
              " '평화집회',\n",
              " '석촌호수',\n",
              " '윌리엄',\n",
              " '싱크탱크',\n",
              " '겨울왕국',\n",
              " '정준영',\n",
              " '여성전용',\n",
              " '금은동',\n",
              " '쿵짝쿵짝',\n",
              " '허구연',\n",
              " '벗어나',\n",
              " '홍보영상',\n",
              " '핵심인물',\n",
              " '묻히',\n",
              " '마티즈',\n",
              " '쉐이',\n",
              " '뉴스보고',\n",
              " '눕방',\n",
              " '때려잡',\n",
              " '셧다운',\n",
              " '년대생',\n",
              " '직장동료',\n",
              " '전여옥',\n",
              " '블랙핑크',\n",
              " '시민의식',\n",
              " '실사판',\n",
              " '폭로한',\n",
              " '처벌받',\n",
              " '탱크보이',\n",
              " '민주노총',\n",
              " '몇년동안',\n",
              " '디카프리오',\n",
              " '전자담배',\n",
              " '됬다',\n",
              " '돌아오',\n",
              " '꽤나',\n",
              " '패스트푸드점',\n",
              " '족치',\n",
              " '추미애',\n",
              " '회사생활',\n",
              " '엠넷',\n",
              " '튕기',\n",
              " '깝치지',\n",
              " '특별출연',\n",
              " '데려가',\n",
              " '당신이',\n",
              " '압수수색',\n",
              " '메달따',\n",
              " '손예진',\n",
              " '대통령중',\n",
              " '민주화운동',\n",
              " '황총리',\n",
              " '추석때',\n",
              " '아시안',\n",
              " '쭉빵',\n",
              " '송승환',\n",
              " '덩치값',\n",
              " '대구시',\n",
              " '씨방새',\n",
              " '양대산맥',\n",
              " '돼지국밥',\n",
              " '똥꼬',\n",
              " '급발진',\n",
              " '빅히트',\n",
              " '특수부',\n",
              " '겟네',\n",
              " '최씨일가',\n",
              " '무선이어폰',\n",
              " '점심메뉴',\n",
              " '패드립',\n",
              " '왓',\n",
              " '맴버쉽',\n",
              " '손해보',\n",
              " '롯데월드타워',\n",
              " '뉴욕타임즈',\n",
              " '눈물나',\n",
              " '책임회피',\n",
              " '팟빵',\n",
              " '큰소리치',\n",
              " '북한도',\n",
              " '태어나',\n",
              " '파파이스',\n",
              " '종북몰이',\n",
              " '공포영화',\n",
              " '풀버전',\n",
              " '소재파악',\n",
              " '괴롭히',\n",
              " '엔시티',\n",
              " '종북빨갱이',\n",
              " '죗값치',\n",
              " '신상공개',\n",
              " '머니투데이',\n",
              " '칠푼아',\n",
              " '쥬리',\n",
              " '뒹굴',\n",
              " '흑역사',\n",
              " '호감순',\n",
              " '여성단체',\n",
              " '줄임말',\n",
              " '신메뉴',\n",
              " '대체복무',\n",
              " '더민주',\n",
              " '팩트체크',\n",
              " '뒷',\n",
              " '유체이탈화법',\n",
              " '십년',\n",
              " '종부세',\n",
              " '쫒아',\n",
              " '철밥통',\n",
              " '바지사장',\n",
              " '키스신',\n",
              " '올리',\n",
              " '닥그네',\n",
              " '여러분이',\n",
              " '금치산자가',\n",
              " '성유리',\n",
              " '오피셜',\n",
              " '야붕이',\n",
              " '봉준호',\n",
              " '퉁치',\n",
              " '싹쓰리',\n",
              " '블록버스터',\n",
              " '떨어뜨리',\n",
              " '미세먼지',\n",
              " '영생교',\n",
              " '안희정',\n",
              " '크리스탈',\n",
              " '젭알',\n",
              " '채동욱을',\n",
              " '싸이코',\n",
              " '뇌물수수죄',\n",
              " '특전사',\n",
              " '팬카페',\n",
              " '톡이',\n",
              " '북한인권결의안',\n",
              " '송지효',\n",
              " '부정입학',\n",
              " '쁘걸',\n",
              " '팔로워',\n",
              " '비선실세',\n",
              " '손담비',\n",
              " '크브스',\n",
              " '한일군사정보협정',\n",
              " '맨시티',\n",
              " '박수홍',\n",
              " '넥스트',\n",
              " '촌스럽',\n",
              " '건강보험공단',\n",
              " '곽도원',\n",
              " '총리지명',\n",
              " '챙기기',\n",
              " '은평구',\n",
              " '로스쿨',\n",
              " '코엑스',\n",
              " '전기차',\n",
              " '종전선언',\n",
              " '박진영',\n",
              " '삼각김밥',\n",
              " '팟캐스트',\n",
              " '딜레이',\n",
              " '두꺼운',\n",
              " '듯하',\n",
              " '옜날',\n",
              " '박근혜가',\n",
              " '김여사',\n",
              " '케사',\n",
              " '일베충',\n",
              " '셜록',\n",
              " '쐐기골',\n",
              " '빡세',\n",
              " '불법체류자',\n",
              " '극장판',\n",
              " '차선변경',\n",
              " '심상정',\n",
              " '노무현대통령',\n",
              " '닌텐도',\n",
              " '로제',\n",
              " '술취한',\n",
              " '돌리',\n",
              " '권선동',\n",
              " '굴리',\n",
              " '찾아보',\n",
              " '로다주',\n",
              " '떠오르',\n",
              " '팥빙수',\n",
              " '횡령혐의',\n",
              " '졷같',\n",
              " '노무현정부때',\n",
              " '드리미',\n",
              " '두테르테가',\n",
              " '두려워',\n",
              " '색갈론',\n",
              " '다이소',\n",
              " '뇬아',\n",
              " '딩가',\n",
              " '명문대',\n",
              " '파이널',\n",
              " '말바꾸기',\n",
              " '폭로글',\n",
              " '통일교',\n",
              " '찌르',\n",
              " '워너비',\n",
              " '벅근혜',\n",
              " '불법주차',\n",
              " '깽판치',\n",
              " '곰탕집',\n",
              " '빅뱅',\n",
              " '뉴스타파',\n",
              " '펠리',\n",
              " '왜저래',\n",
              " '에버랜드',\n",
              " '들어가',\n",
              " '효도르',\n",
              " '최태민',\n",
              " '교보문고',\n",
              " '때문이',\n",
              " '중고나라',\n",
              " '딜리버리',\n",
              " '지정생존자',\n",
              " '몇개',\n",
              " '네스프레소',\n",
              " '레노버',\n",
              " '캔맥주',\n",
              " '최순실의',\n",
              " '롯데백화점',\n",
              " '국가안보',\n",
              " '어버이연합',\n",
              " '물류센터',\n",
              " '누구하나',\n",
              " '클래스',\n",
              " '사이버',\n",
              " '전지현',\n",
              " '뚫리',\n",
              " '청담동',\n",
              " '브렉시트',\n",
              " '희망고문',\n",
              " '넋나간',\n",
              " '얍삽하',\n",
              " '브로드밴드',\n",
              " '사이비종교',\n",
              " '퇴근시간',\n",
              " '블루레이',\n",
              " '클럽하우스',\n",
              " '팬트하우스',\n",
              " '용인시',\n",
              " '듣보잡',\n",
              " '멈추',\n",
              " '눈썹문신',\n",
              " '강남구',\n",
              " '강남스타일',\n",
              " '닭년아',\n",
              " '펌글',\n",
              " '아무생각없이',\n",
              " '건드리',\n",
              " '통일부장관',\n",
              " '쫓아가',\n",
              " '밀어붙이',\n",
              " '고영태',\n",
              " '껌씹',\n",
              " '국제대회',\n",
              " '허위사실',\n",
              " '어느나라',\n",
              " '두번째',\n",
              " '군통수권',\n",
              " '유튜브',\n",
              " '장시호',\n",
              " '최순실씨',\n",
              " '토트넘',\n",
              " '나무위키',\n",
              " '택배기사',\n",
              " '무슨생각',\n",
              " '차태현',\n",
              " '김문수',\n",
              " '코스프레',\n",
              " '프로게이머',\n",
              " '쇼쇼쇼',\n",
              " '터지',\n",
              " '신하균',\n",
              " '팽당',\n",
              " '장우진',\n",
              " '집어넣',\n",
              " '울나라',\n",
              " '타임스퀘어',\n",
              " '별로네',\n",
              " '중국산',\n",
              " '우리집',\n",
              " '페이커',\n",
              " '강민경',\n",
              " '이명박',\n",
              " '쇼케이스',\n",
              " '지마켓',\n",
              " '구기종목',\n",
              " '디씨',\n",
              " '도대체가',\n",
              " '방산비리',\n",
              " '속지마라',\n",
              " '직무유기',\n",
              " '록시',\n",
              " '갯수',\n",
              " '휩쓸',\n",
              " '세계일보',\n",
              " '핑계대고',\n",
              " '노무현',\n",
              " '싀바',\n",
              " '절대안',\n",
              " '죠스',\n",
              " '한일군사보호협정',\n",
              " '페이스북',\n",
              " '룸싸롱',\n",
              " '귀요미',\n",
              " '일본방송',\n",
              " '명예회복',\n",
              " '삥뜯기',\n",
              " '역대급인',\n",
              " '전효성',\n",
              " '왜케',\n",
              " '부녀회장',\n",
              " '호빠선수',\n",
              " '클라스',\n",
              " '받아들이',\n",
              " '쉐끼들',\n",
              " '손혜원',\n",
              " '프라다',\n",
              " '음주단속',\n",
              " '차은택',\n",
              " '모든걸',\n",
              " '보이스피싱',\n",
              " '국방부장관',\n",
              " '난리부르스',\n",
              " '상황파악',\n",
              " '외노자',\n",
              " '땡큐',\n",
              " '인기가요',\n",
              " '돈주고',\n",
              " '축구도',\n",
              " '뷸러',\n",
              " '싱가폴',\n",
              " '응원소리',\n",
              " '카트라이더',\n",
              " '생일파티',\n",
              " '송로버섯',\n",
              " '년생이',\n",
              " '탁재훈',\n",
              " '땡깡',\n",
              " '야구방',\n",
              " '네이버웹툰',\n",
              " '휘두르',\n",
              " '파리바게트',\n",
              " '차이나',\n",
              " '댓글알바',\n",
              " '삼성화재',\n",
              " '국민세금',\n",
              " '킹덤',\n",
              " '재결집',\n",
              " '죄순실',\n",
              " '좆본',\n",
              " '포스코',\n",
              " '김건모',\n",
              " '김제동',\n",
              " '뮤직뱅크',\n",
              " '린다김',\n",
              " '쿨하',\n",
              " '길가다가',\n",
              " '크루즈선',\n",
              " '국방비리',\n",
              " '피규어',\n",
              " '쌍용',\n",
              " '여자연예인',\n",
              " '멀티골',\n",
              " '증거인멸시간',\n",
              " '연금복권',\n",
              " '댓통령',\n",
              " '조윤선',\n",
              " '몇번째',\n",
              " '초딩',\n",
              " '텔레토비',\n",
              " '군면제',\n",
              " '비매너',\n",
              " '윤서인',\n",
              " '바그네',\n",
              " '능지처참해',\n",
              " '왜그리',\n",
              " '학창시절',\n",
              " '쿠테타',\n",
              " '총리추천',\n",
              " '어메이징',\n",
              " '몇시간',\n",
              " '양대인',\n",
              " '퍼포먼스',\n",
              " '리테일',\n",
              " '임팩트',\n",
              " '촌스',\n",
              " '대한민국국민',\n",
              " '내려가',\n",
              " '퍼스트',\n",
              " '대포폰',\n",
              " '건강악화',\n",
              " '길가다',\n",
              " '쯧쯧쯧',\n",
              " '여자친구',\n",
              " '놔두',\n",
              " '카드뉴스',\n",
              " '우리팀',\n",
              " '닉네임',\n",
              " '추미애씨',\n",
              " '쉐이들',\n",
              " '썻',\n",
              " '운영자',\n",
              " '헌정중단',\n",
              " '틀림없',\n",
              " '타임라인',\n",
              " '빕스',\n",
              " '퀄리티',\n",
              " '승부조작',\n",
              " '얻어먹',\n",
              " '펼치',\n",
              " '박근혜게이트',\n",
              " '레시피',\n",
              " '늘어놓',\n",
              " '창피해',\n",
              " '뒤집어씌우',\n",
              " '댓글달',\n",
              " '도핑검사',\n",
              " '야당아',\n",
              " '잡아가',\n",
              " '대장내시경',\n",
              " '서인국',\n",
              " '졋다',\n",
              " '혈압주의',\n",
              " '싸그리',\n",
              " '뷰티',\n",
              " '첼시',\n",
              " '다시한번',\n",
              " '눈물남',\n",
              " '측근들이',\n",
              " '민노총',\n",
              " '뚜레쥬르',\n",
              " '저정도',\n",
              " '젝스키스',\n",
              " '챔피언스리그',\n",
              " '흐리',\n",
              " '하나만',\n",
              " '셧다운제',\n",
              " '케미',\n",
              " '문건유출',\n",
              " '소리소문없이',\n",
              " '무한도전',\n",
              " '패널티',\n",
              " '층간소음',\n",
              " '뭔소리',\n",
              " '진행중인',\n",
              " '심박수',\n",
              " '북한허락',\n",
              " '양념치킨',\n",
              " '욱일기',\n",
              " '긴급속보',\n",
              " '개인정보',\n",
              " '대선후보',\n",
              " '신경치료',\n",
              " '럼아',\n",
              " '두가지',\n",
              " '임시공휴일',\n",
              " '권상우',\n",
              " '설문조사',\n",
              " '미국선수',\n",
              " '후회없이',\n",
              " '겔럭시',\n",
              " '솔로곡',\n",
              " '음주운전',\n",
              " '웨이보',\n",
              " '뮤지션',\n",
              " '급공무원',\n",
              " '오세훈',\n",
              " '임기기간동안',\n",
              " '했으',\n",
              " '상남자',\n",
              " '돋보',\n",
              " '뱉어내',\n",
              " '야구선수',\n",
              " '많이들',\n",
              " '끌어내리',\n",
              " '더민주의',\n",
              " '홀리데이',\n",
              " '좁',\n",
              " '샥스',\n",
              " '퇴진운동',\n",
              " '꼬리자르기',\n",
              " '한일군사협정',\n",
              " '울동네',\n",
              " '남자아이돌',\n",
              " '냇다',\n",
              " '박관천',\n",
              " '쩍벌',\n",
              " '앰창',\n",
              " '글로벌',\n",
              " '헌법상',\n",
              " '해설위원',\n",
              " '란걸',\n",
              " '감상평',\n",
              " '인스타',\n",
              " '참전용사',\n",
              " '의혹제기',\n",
              " '자진사퇴',\n",
              " '리액션',\n",
              " '꼭조',\n",
              " '남자친구',\n",
              " '로켓배송',\n",
              " '구조조정',\n",
              " '올단두대',\n",
              " '트리플',\n",
              " '차우찬',\n",
              " '빚내서',\n",
              " '유플러스',\n",
              " '공공기관',\n",
              " '언젠간',\n",
              " '세종대왕',\n",
              " '천둥번개',\n",
              " '군복무',\n",
              " '김소연',\n",
              " '이새끼',\n",
              " '데리',\n",
              " '그정도',\n",
              " '토이스토리',\n",
              " '최경위',\n",
              " '뻣고',\n",
              " '뺏기',\n",
              " '일어나',\n",
              " '또라이',\n",
              " '떡락',\n",
              " '손혜원의원',\n",
              " '헤롱헤롱',\n",
              " '홧병나',\n",
              " '김대중',\n",
              " '현대자동차',\n",
              " '누군지',\n",
              " '허위사실유포',\n",
              " '축구선수',\n",
              " '수상소감',\n",
              " '임기내내',\n",
              " '빠져나가',\n",
              " '현대차',\n",
              " '잠잠해지',\n",
              " '빈부격차',\n",
              " '해경해체',\n",
              " '박근혜씨',\n",
              " '조기종료',\n",
              " '접촉사고',\n",
              " '며칠간',\n",
              " '재드래곤',\n",
              " '학교폭력',\n",
              " '속상하',\n",
              " '엥간',\n",
              " '히스토리',\n",
              " '롯데타워',\n",
              " '엠사',\n",
              " '둘중',\n",
              " '질문좀',\n",
              " '법무부장관',\n",
              " '엄마아빠',\n",
              " '대국민담화',\n",
              " '펙트',\n",
              " '셧다',\n",
              " '뱅기',\n",
              " '박찬호',\n",
              " '커터칼',\n",
              " '옆나라',\n",
              " '콜센터',\n",
              " '삥뜯',\n",
              " '근본적인',\n",
              " '뛰어나',\n",
              " '트와이스',\n",
              " '뱉어',\n",
              " '섹시',\n",
              " '초보운전',\n",
              " '병역비리',\n",
              " '황장엽',\n",
              " '역대급',\n",
              " '요구사항',\n",
              " '케백수',\n",
              " '국기문란',\n",
              " '백지영',\n",
              " '바르샤',\n",
              " '포켓몬스터',\n",
              " '코카콜라',\n",
              " '쌔',\n",
              " '감방가',\n",
              " '째려보',\n",
              " '정유라가',\n",
              " '프린세스메이커',\n",
              " '음악방송',\n",
              " '병신년',\n",
              " '마리텔',\n",
              " '붐비',\n",
              " '방금전',\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DSeeL0TG6Gz",
        "outputId": "4a5d1556-1d44-4d3c-9179-fe34b5d9e02a"
      },
      "source": [
        "!pip install jamo"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jamo in /usr/local/lib/python3.7/dist-packages (0.4.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRJH4-ksG87L"
      },
      "source": [
        "from jamo import h2j, j2hcj \n",
        "def get_jongsung_TF(sample_text): \n",
        "  sample_text_list = list(sample_text) \n",
        "  last_word = sample_text_list[-1] \n",
        "  last_word_jamo_list = list(j2hcj(h2j(last_word))) \n",
        "  last_jamo = last_word_jamo_list[-1] \n",
        "  jongsung_TF = \"T\" \n",
        "  if last_jamo in ['ㅏ', 'ㅑ', 'ㅓ', 'ㅕ', 'ㅗ', 'ㅛ', 'ㅜ', 'ㅠ', 'ㅡ', 'ㅣ', 'ㅘ', 'ㅚ', 'ㅙ', 'ㅝ', 'ㅞ', 'ㅢ', 'ㅐ,ㅔ', 'ㅟ', 'ㅖ', 'ㅒ']: \n",
        "    jongsung_TF = \"F\" \n",
        "    return jongsung_TF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ae-vA3VG9x_"
      },
      "source": [
        "with open(\"/content/mecab-ko-dic-2.1.1-20180720/user-dic/nnp.csv\", 'r', encoding='utf-8') as f: \n",
        "  file_data = f.readlines() \n",
        "  for word in word_list: \n",
        "    jongsung_TF = get_jongsung_TF(word) \n",
        "    line = '{},,,,NNP,*,{},{},*,*,*,*,*\\n'.format(word, jongsung_TF, word) \n",
        "    file_data.append(line)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBGJ2II0HAyZ",
        "outputId": "5ccf5078-6228-4163-e8e0-3bd9d137b42e"
      },
      "source": [
        "slangs = pd.read_csv(\"/content/total_answers.csv\")\n",
        "len(slangs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2006"
            ]
          },
          "metadata": {},
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w_3xyIzHDlV",
        "outputId": "19f9548b-9ab3-4083-8b8c-bd836cd0fca8"
      },
      "source": [
        "with open(\"./user-dic/nnp.csv\", 'r', encoding='utf-8') as f:\n",
        "  file_new = f.readlines() \n",
        "file_new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['대우,,,,NNP,*,F,대우,*,*,*,*,*\\n', '구글,,,,NNP,*,T,구글,*,*,*,*,*\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hJXAkYsQSLp",
        "outputId": "2ee53acb-f00f-44c0-8005-0750f058b5e0"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclocal.m4       feature.def                 model.def          right-id.def\n",
            "AUTHORS          Foreign.csv                 NEWS               Symbol.csv\n",
            "\u001b[0m\u001b[01;32mautogen.sh\u001b[0m*      Group.csv                   NNBC.csv           \u001b[01;34mtools\u001b[0m/\n",
            "\u001b[01;34mautom4te.cache\u001b[0m/  Hanja.csv                   NNB.csv            unk.def\n",
            "ChangeLog        IC.csv                      NNG.csv            \u001b[01;34muser-dic\u001b[0m/\n",
            "char.def         Inflect.csv                 NNP.csv            VA.csv\n",
            "\u001b[01;32mclean\u001b[0m*           INSTALL                     NorthKorea.csv     VCN.csv\n",
            "CoinedWord.csv   \u001b[01;32minstall-sh\u001b[0m*                 NP.csv             VCP.csv\n",
            "config.log       J.csv                       NR.csv             VV.csv\n",
            "\u001b[01;32mconfig.status\u001b[0m*   left-id.def                 \u001b[01;34mnsmc\u001b[0m/              VX.csv\n",
            "\u001b[01;32mconfigure\u001b[0m*       MAG.csv                     Person-actor.csv   Wikipedia.csv\n",
            "configure.ac     MAJ.csv                     Person.csv         XPN.csv\n",
            "COPYING          Makefile                    Place-address.csv  XR.csv\n",
            "dicrc            Makefile.am                 Place.csv          XSA.csv\n",
            "EC.csv           Makefile.in                 Place-station.csv  XSN.csv\n",
            "EF.csv           matrix.def                  pos-id.def         XSV.csv\n",
            "EP.csv           \u001b[01;34mMecab-ko-for-Google-Colab\u001b[0m/  Preanalysis.csv\n",
            "ETM.csv          \u001b[01;32mmissing\u001b[0m*                    README\n",
            "ETN.csv          MM.csv                      rewrite.def\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh7WyIZcQUHA",
        "outputId": "8afbfb71-8a9c-46ad-ded3-72b3c166144b"
      },
      "source": [
        "ls tools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;32madd-userdic.sh\u001b[0m*  \u001b[01;32mconvert_for_using_store.sh\u001b[0m*  \u001b[01;32mmecab-bestn.sh\u001b[0m*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyzmeNmnQWVq",
        "outputId": "ed5e8f52-5b47-4fff-a5bb-a46357725ddc"
      },
      "source": [
        "!bash ./tools/add-userdic.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generating userdic...\n",
            "nnp.csv\n",
            "/content/mecab-ko-dic-2.1.1-20180720/tools/../model.def is not a binary model. reopen it as text mode...\n",
            "dictionary.cpp(171) [property.open(param)] \n",
            "person.csv\n",
            "/content/mecab-ko-dic-2.1.1-20180720/tools/../model.def is not a binary model. reopen it as text mode...\n",
            "dictionary.cpp(171) [property.open(param)] \n",
            "place.csv\n",
            "/content/mecab-ko-dic-2.1.1-20180720/tools/../model.def is not a binary model. reopen it as text mode...\n",
            "dictionary.cpp(171) [property.open(param)] \n",
            "/bin/bash ./config.status --recheck\n",
            "running CONFIG_SHELL=/bin/bash /bin/bash ./configure --no-create --no-recursion\n",
            "./configure: line 1696: syntax error near unexpected token `mecab-ko-dic,'\n",
            "./configure: line 1696: `AM_INIT_AUTOMAKE(mecab-ko-dic, 2.0.0)'\n",
            "Makefile:233: recipe for target 'config.status' failed\n",
            "make: *** [config.status] Error 2\n",
            "/bin/bash ./config.status --recheck\n",
            "running CONFIG_SHELL=/bin/bash /bin/bash ./configure --no-create --no-recursion\n",
            "./configure: line 1696: syntax error near unexpected token `mecab-ko-dic,'\n",
            "./configure: line 1696: `AM_INIT_AUTOMAKE(mecab-ko-dic, 2.0.0)'\n",
            "Makefile:233: recipe for target 'config.status' failed\n",
            "make: *** [config.status] Error 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOzOwcQGQgwr",
        "outputId": "e5777bd0-7712-4057-9f31-7ad2325ebdb2"
      },
      "source": [
        "!make install"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash ./config.status --recheck\n",
            "running CONFIG_SHELL=/bin/bash /bin/bash ./configure --no-create --no-recursion\n",
            "./configure: line 1696: syntax error near unexpected token `mecab-ko-dic,'\n",
            "./configure: line 1696: `AM_INIT_AUTOMAKE(mecab-ko-dic, 2.0.0)'\n",
            "Makefile:233: recipe for target 'config.status' failed\n",
            "make: *** [config.status] Error 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdkWKXaFQnPT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiwbLpGTHkYh"
      },
      "source": [
        "# Mecab 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jRFN7DdE7RL",
        "outputId": "4e1f85a2-991b-4e30-885a-a266963160d4"
      },
      "source": [
        "import konlpy\n",
        "konlpy.tag.Mecab().morphs('아버지가방에들어가신다')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['아버지', '가', '방', '에', '들어가', '신다']"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkt5_BvrFabe",
        "outputId": "4f72e7a7-4f1c-4a91-cab9-d5a10daa8870"
      },
      "source": [
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mecab-ko-for-Google-Colab'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Counting objects: 100% (91/91), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 91 (delta 43), reused 22 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (91/91), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMC2s6nDFvuG",
        "outputId": "8688a208-63a2-4f45-fed9-c2a456f942f6"
      },
      "source": [
        "cd Mecab-ko-for-Google-Colab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/mecab-ko-dic-2.1.1-20180720/Mecab-ko-for-Google-Colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JFkpKTiFzJE",
        "outputId": "fae706bc-9beb-40db-958f-280294e2ea52"
      },
      "source": [
        "! bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing konlpy.....\n",
            "Requirement already satisfied: konlpy in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: beautifulsoup4==4.6.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.6.0)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: JPype1>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.3.0)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from konlpy) (0.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Done\n",
            "Installing mecab-0.996-ko-0.9.2.tar.gz.....\n",
            "Downloading mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "--2021-08-24 07:57:55--  https://bitbucket.org/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c2:513, 2406:da00:ff00::22c3:9b0a, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=6VM60A7n%2Bw1s7TJEi7wC4Mq0j74%3D&Expires=1629792819&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-08-24 07:57:55--  https://bbuseruploads.s3.amazonaws.com/eunjeon/mecab-ko/downloads/mecab-0.996-ko-0.9.2.tar.gz?Signature=6VM60A7n%2Bw1s7TJEi7wC4Mq0j74%3D&Expires=1629792819&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=null&response-content-disposition=attachment%3B%20filename%3D%22mecab-0.996-ko-0.9.2.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.236.203\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.236.203|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1414979 (1.3M) [application/x-tar]\n",
            "Saving to: ‘mecab-0.996-ko-0.9.2.tar.gz.1’\n",
            "\n",
            "mecab-0.996-ko-0.9. 100%[===================>]   1.35M  3.50MB/s    in 0.4s    \n",
            "\n",
            "2021-08-24 07:57:56 (3.50 MB/s) - ‘mecab-0.996-ko-0.9.2.tar.gz.1’ saved [1414979/1414979]\n",
            "\n",
            "Done\n",
            "Unpacking mecab-0.996-ko-0.9.2.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-0.996-ko-0.9.2.......\n",
            "installing mecab-0.996-ko-0.9.2.tar.gz........\n",
            "configure\n",
            "make\n",
            "make check\n",
            "make install\n",
            "ldconfig\n",
            "Done\n",
            "Change Directory to /content\n",
            "Downloading mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "from https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "--2021-08-24 07:58:09--  https://bitbucket.org/eunjeon/mecab-ko-dic/downloads/mecab-ko-dic-2.1.1-20180720.tar.gz\n",
            "Resolving bitbucket.org (bitbucket.org)... 104.192.141.1, 2406:da00:ff00::22c2:513, 2406:da00:ff00::22c3:9b0a, ...\n",
            "Connecting to bitbucket.org (bitbucket.org)|104.192.141.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=86iPU7jX%2BOxZNx33fLx4JKTNj2s%3D&Expires=1629792894&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None [following]\n",
            "--2021-08-24 07:58:10--  https://bbuseruploads.s3.amazonaws.com/a4fcd83e-34f1-454e-a6ac-c242c7d434d3/downloads/b5a0c703-7b64-45ed-a2d7-180e962710b6/mecab-ko-dic-2.1.1-20180720.tar.gz?Signature=86iPU7jX%2BOxZNx33fLx4JKTNj2s%3D&Expires=1629792894&AWSAccessKeyId=AKIA6KOSE3BNJRRFUUX6&versionId=tzyxc1TtnZU_zEuaaQDGN4F76hPDpyFq&response-content-disposition=attachment%3B%20filename%3D%22mecab-ko-dic-2.1.1-20180720.tar.gz%22&response-content-encoding=None\n",
            "Resolving bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)... 52.216.206.115\n",
            "Connecting to bbuseruploads.s3.amazonaws.com (bbuseruploads.s3.amazonaws.com)|52.216.206.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 49775061 (47M) [application/x-tar]\n",
            "Saving to: ‘mecab-ko-dic-2.1.1-20180720.tar.gz.1’\n",
            "\n",
            "mecab-ko-dic-2.1.1- 100%[===================>]  47.47M  30.0MB/s    in 1.6s    \n",
            "\n",
            "2021-08-24 07:58:12 (30.0 MB/s) - ‘mecab-ko-dic-2.1.1-20180720.tar.gz.1’ saved [49775061/49775061]\n",
            "\n",
            "Done\n",
            "Unpacking  mecab-ko-dic-2.1.1-20180720.tar.gz.......\n",
            "Done\n",
            "Change Directory to mecab-ko-dic-2.1.1-20180720\n",
            "Done\n",
            "installing........\n",
            "configure\n",
            "make\n",
            "make install\n",
            "apt-get update\n",
            "apt-get upgrade\n",
            "apt install curl\n",
            "apt install git\n",
            "bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)\n",
            "Done\n",
            "Successfully Installed\n",
            "Now you can use Mecab\n",
            "from konlpy.tag import Mecab\n",
            "mecab = Mecab()\n",
            "사용자 사전 추가 방법 : https://bit.ly/3k0ZH53\n",
            "NameError: name 'Tagger' is not defined 오류 발생 시 런타임을 재실행 해주세요\n",
            "블로그에 해결 방법을 남겨주신 tana님 감사합니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFwKYXQYF3YR",
        "outputId": "50a31d33-4317-4dc5-8f04-bf27875aada3"
      },
      "source": [
        "from konlpy.tag import Mecab \n",
        "mecab = Mecab() \n",
        "text = u\"\"\"좆본 떡락 에바인데\"\"\" \n",
        "nouns = mecab.nouns(text) \n",
        "print(nouns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['좆', '떡', '에바']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGemhluQWixX"
      },
      "source": [
        "#['[CLS]', '아', '더', '##빙', '.', '.', '진', '##짜', '짜', '##증', '##나', '##네', '##요', '목', '##소', '##리', '[SEP]']\n",
        "mecab = Mecab()\n",
        "total_morph = []\n",
        "first = True\n",
        "for sentence in train['document'][:150994]:\n",
        "    morph = []\n",
        "    if first == True:\n",
        "        morph.append('[CLS]')\n",
        "        first = False\n",
        "    else:\n",
        "        morph.append('[SEP]')\n",
        "    for st in sentence.split(\" \"):\n",
        "        count = 0\n",
        "        for token in mecab.morphs(st):\n",
        "            tk = token\n",
        "            if count > 0:\n",
        "                tk = \"##\" + tk\n",
        "                morph.append(tk)\n",
        "            else:\n",
        "                morph.append(tk)\n",
        "                count += 1\n",
        "    total_morph.append(morph)            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ3_qrkJLcYd"
      },
      "source": [
        "# txt_file = open(\"/content/mecabb.txt\", 'w')  # 텍스트 파일을 쓰기 모드로 생성\n",
        "\n",
        "# for token in output_tokens:\n",
        "#   txt_file.write(token + '\\n')\n",
        "\n",
        "# txt_file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAgd9RzWdxOz"
      },
      "source": [
        "# **전처리-훈련셋**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xWk6yqWGDGD",
        "outputId": "f14c4def-1e7c-491c-ca43-1ddc7febed23"
      },
      "source": [
        "# 라벨 추출\n",
        "labels = train['label'].values\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmBrFhVqFZki"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ1N6IAbGHtR",
        "outputId": "ceb2053c-9cab-4bf7-f384-a94da8772320"
      },
      "source": [
        "# 입력 토큰의 최대 시퀀스 길이\n",
        "MAX_LEN = 128\n",
        "\n",
        "# 토큰을 숫자 인덱스로 변환\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in total_morph]\n",
        "\n",
        "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "input_ids[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   101,   9519,   9074, 119005, 110864, 110864,    100,    100,\n",
              "        16439,    100,    100,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0,\n",
              "            0,      0,      0,      0,      0,      0,      0,      0])"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umH6RXCCGLYM",
        "outputId": "3bad1805-3ba8-43c1-b3fa-d2fa03cfc623"
      },
      "source": [
        "# 어텐션 마스크 초기화\n",
        "attention_masks = []\n",
        "\n",
        "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "print(attention_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGcd20UyGNLB",
        "outputId": "26c6f1dc-9087-48c6-c9df-81ce79f4ef62"
      },
      "source": [
        "# 훈련셋과 검증셋으로 분리\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,\n",
        "                                                                                    labels, \n",
        "                                                                                    random_state=2018, \n",
        "                                                                                    test_size=0.1)\n",
        "\n",
        "# 어텐션 마스크를 훈련셋과 검증셋으로 분리\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, \n",
        "                                                       input_ids,\n",
        "                                                       random_state=2018, \n",
        "                                                       test_size=0.1)\n",
        "\n",
        "# 데이터를 파이토치의 텐서로 변환\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks = torch.tensor(validation_masks)\t\t\t\t\n",
        "\n",
        "print(train_inputs[0])\n",
        "print(train_labels[0])\n",
        "print(train_masks[0])\n",
        "print(validation_inputs[0])\n",
        "print(validation_labels[0])\n",
        "print(validation_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([   102,    100,  15891, 119136,  11018,    100, 110864, 110864,    100,\n",
            "           100,    100,  11102,    100,    100,   8984,   9596, 119170,  11664,\n",
            "        119192, 118728,    100,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "tensor([   102,   8984,    100,  10709, 110881,  48205,  10954,  12092,  10530,\n",
            "         73306,    100,    100, 119113, 119118,  11903,   9144,  42428,  11018,\n",
            "           100,  11467,  19105,   9356,  11018,  14153,    100,  59894,    100,\n",
            "           100,    100,    100,  62200,  82564,    100,  10459,    100,  14801,\n",
            "         12030,    100,  11018,    100,  11513,    100,  22096,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
            "             0,      0])\n",
            "tensor(0)\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
            "        1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ghPGlh4GPmi"
      },
      "source": [
        "# 배치 사이즈\n",
        "batch_size = 32\n",
        "\n",
        "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
        "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjWLFTwvdriL"
      },
      "source": [
        "# **전처리-테스트셋**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mijIvq6GVfn",
        "outputId": "edf7227e-ad6a-49df-fdbe-59f237e0a537"
      },
      "source": [
        "# 라벨 추출\n",
        "labels = test['label'].values\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9mDmHipGUtj"
      },
      "source": [
        "mecab = Mecab()\n",
        "total_morph = []\n",
        "first = True\n",
        "for sentence in test['document']:\n",
        "    morph = []\n",
        "    if first == True:\n",
        "        morph.append('[CLS]')\n",
        "        first = False\n",
        "    else:\n",
        "        morph.append('[SEP]')\n",
        "    for st in sentence.split(\" \"):\n",
        "        count = 0\n",
        "        for token in mecab.morphs(st):\n",
        "            tk = token\n",
        "            if count > 0:\n",
        "                tk = \"##\" + tk\n",
        "                morph.append(tk)\n",
        "            else:\n",
        "                morph.append(tk)\n",
        "                count += 1\n",
        "    total_morph.append(morph)            "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7hP22uhGYau",
        "outputId": "0bbdbcad-79df-4cc4-a117-a4bcfd445f91"
      },
      "source": [
        "# 입력 토큰의 최대 시퀀스 길이\n",
        "MAX_LEN = 128\n",
        "\n",
        "# 토큰을 숫자 인덱스로 변환\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in total_morph]\n",
        "\n",
        "# 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "input_ids[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 101, 8911,  100,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nf5uvwLPGacx",
        "outputId": "040181e8-5df7-4565-fb6d-d00c9a6ac94c"
      },
      "source": [
        "# 어텐션 마스크 초기화\n",
        "attention_masks = []\n",
        "\n",
        "# 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "# 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "for seq in input_ids:\n",
        "    seq_mask = [float(i>0) for i in seq]\n",
        "    attention_masks.append(seq_mask)\n",
        "\n",
        "print(attention_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14dPPYY5GcpR",
        "outputId": "c07422a6-29bf-45d7-81e1-f98ea00c0552"
      },
      "source": [
        "# 데이터를 파이토치의 텐서로 변환\n",
        "test_inputs = torch.tensor(input_ids)\n",
        "test_labels = torch.tensor(labels)\n",
        "test_masks = torch.tensor(attention_masks)\n",
        "\n",
        "print(test_inputs[0])\n",
        "print(test_labels[0])\n",
        "print(test_masks[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 101, 8911,  100,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0,    0,    0,    0,    0,    0,    0])\n",
            "tensor(1)\n",
            "tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTv8seZhGe44"
      },
      "source": [
        "# 배치 사이즈\n",
        "batch_size = 32\n",
        "\n",
        "# 파이토치의 DataLoader로 입력, 마스크, 라벨을 묶어 데이터 설정\n",
        "# 학습시 배치 사이즈 만큼 데이터를 가져옴\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = RandomSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp6bkY_xgBbO"
      },
      "source": [
        "# **모델생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AWaho-lGhnH",
        "outputId": "0aae7bb8-2ac2-491e-ca9d-7edee95ac435"
      },
      "source": [
        "# GPU 디바이스 이름 구함\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# GPU 디바이스 이름 검사\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4uwDjNtGkME",
        "outputId": "9044ae5e-53db-4f05-9e8a-c5429c7e09ea"
      },
      "source": [
        "# 디바이스 설정\n",
        "if torch.cuda.is_available():    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('No GPU available, using the CPU instead.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZpaYfPS4GmKM",
        "outputId": "6051f039-76ca-4921-98f4-aaee30f341c3"
      },
      "source": [
        "# 분류를 위한 BERT 모델 생성\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcBnXzGSGrmL"
      },
      "source": [
        "# 옵티마이저 설정\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # 학습률\n",
        "                  eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값\n",
        "                )\n",
        "\n",
        "# 에폭수\n",
        "epochs = 4\n",
        "\n",
        "# 총 훈련 스텝 : 배치반복 횟수 * 에폭\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# 처음에 학습률을 조금씩 변화시키는 스케줄러 생성\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3U4CjSsgmEO"
      },
      "source": [
        "# **모델 학습**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taytRBcMGsPc"
      },
      "source": [
        "# 정확도 계산 함수\n",
        "def flat_accuracy(preds, labels):\n",
        "    \n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9tJ-0XAGutl"
      },
      "source": [
        "# 시간 표시 함수\n",
        "def format_time(elapsed):\n",
        "\n",
        "    # 반올림\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # hh:mm:ss으로 형태 변경\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "tV_PTQuvGz-c",
        "outputId": "f97daf72-f2f8-4c2e-920a-0628f2b5098d"
      },
      "source": [
        "# 재현을 위해 랜덤시드 고정\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# 그래디언트 초기화\n",
        "model.zero_grad()\n",
        "\n",
        "# 에폭만큼 반복\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # 시작 시간 설정\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 로스 초기화\n",
        "    total_loss = 0\n",
        "\n",
        "    # 훈련모드로 변경\n",
        "    model.train()\n",
        "        \n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # 경과 정보 표시\n",
        "        if step % 500 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Forward 수행                \n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels)\n",
        "        \n",
        "        # 로스 구함\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # 총 로스 계산\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward 수행으로 그래디언트 계산\n",
        "        loss.backward()\n",
        "\n",
        "        # 그래디언트 클리핑\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # 그래디언트를 통해 가중치 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 스케줄러로 학습률 감소\n",
        "        scheduler.step()\n",
        "\n",
        "        # 그래디언트 초기화\n",
        "        model.zero_grad()\n",
        "\n",
        "    # 평균 로스 계산\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    #시작 시간 설정\n",
        "    t0 = time.time()\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 변수 초기화\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "    for batch in validation_dataloader:\n",
        "        # 배치를 GPU에 넣음\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # 배치에서 데이터 추출\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # 그래디언트 계산 안함\n",
        "        with torch.no_grad():     \n",
        "            # Forward 수행\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # 로스 구함\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # CPU로 데이터 이동\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "  Batch   500  of  4,247.    Elapsed: 0:05:57.\n",
            "  Batch 1,000  of  4,247.    Elapsed: 0:12:06.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-105-44dbd305962a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Backward 수행으로 그래디언트 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# 그래디언트 클리핑\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI-IChSlG08K"
      },
      "source": [
        "#시작 시간 설정\n",
        "t0 = time.time()\n",
        "\n",
        "# 평가모드로 변경\n",
        "model.eval()\n",
        "\n",
        "# 변수 초기화\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "# 데이터로더에서 배치만큼 반복하여 가져옴\n",
        "for step, batch in enumerate(test_dataloader):\n",
        "    # 경과 정보 표시\n",
        "    if step % 100 == 0 and not step == 0:\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
        "\n",
        "    # 배치를 GPU에 넣음\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # 배치에서 데이터 추출\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # 그래디언트 계산 안함\n",
        "    with torch.no_grad():     \n",
        "        # Forward 수행\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "    \n",
        "    # 로스 구함\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # CPU로 데이터 이동\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    \n",
        "    # 출력 로짓과 라벨을 비교하여 정확도 계산\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "print(\"\")\n",
        "print(\"Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "print(\"Test took: {:}\".format(format_time(time.time() - t0)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_eodwEaG32_"
      },
      "source": [
        "# 입력 데이터 변환\n",
        "def convert_input_data(sentences):\n",
        "\n",
        "    # BERT의 토크나이저로 문장을 토큰으로 분리\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "\n",
        "    # 입력 토큰의 최대 시퀀스 길이\n",
        "    MAX_LEN = 128\n",
        "\n",
        "    # 토큰을 숫자 인덱스로 변환\n",
        "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
        "    \n",
        "    # 문장을 MAX_LEN 길이에 맞게 자르고, 모자란 부분을 패딩 0으로 채움\n",
        "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "    # 어텐션 마스크 초기화\n",
        "    attention_masks = []\n",
        "\n",
        "    # 어텐션 마스크를 패딩이 아니면 1, 패딩이면 0으로 설정\n",
        "    # 패딩 부분은 BERT 모델에서 어텐션을 수행하지 않아 속도 향상\n",
        "    for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "\n",
        "    # 데이터를 파이토치의 텐서로 변환\n",
        "    inputs = torch.tensor(input_ids)\n",
        "    masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return inputs, masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKpq3y0fG6O1"
      },
      "source": [
        "# 문장 테스트\n",
        "def test_sentences(sentences):\n",
        "\n",
        "    # 평가모드로 변경\n",
        "    model.eval()\n",
        "\n",
        "    # 문장을 입력 데이터로 변환\n",
        "    inputs, masks = convert_input_data(sentences)\n",
        "\n",
        "    # 데이터를 GPU에 넣음\n",
        "    b_input_ids = inputs.to(device)\n",
        "    b_input_mask = masks.to(device)\n",
        "            \n",
        "    # 그래디언트 계산 안함\n",
        "    with torch.no_grad():     \n",
        "        # Forward 수행\n",
        "        outputs = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    # 로스 구함\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # CPU로 데이터 이동\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95wy8Ap5G8rd"
      },
      "source": [
        "logits = test_sentences(['소신발언) 앰창두 싫어하는데 리그앙 개병신좆밥 리그맞음'])\n",
        "\n",
        "print(logits)\n",
        "print(np.argmax(logits))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coE-of1xjrY_"
      },
      "source": [
        "# lis = []\n",
        "# for sent in sl['document'][1000:]:\n",
        "#   logits = test_sentences([sent])\n",
        "#   lis.append(np.argmax(logits))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsZLerGCliJf"
      },
      "source": [
        "# from sklearn.metrics import accuracy_score\n",
        "# accuracy_score(lis, sl['label'][1000:])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}